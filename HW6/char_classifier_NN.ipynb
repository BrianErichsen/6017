{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HW 6: Character classification using KNN with PyTorch\n",
    "\n",
    "Author:\n",
    "</b> Brian Erichsen Fagundes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data acquision + clenup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# loads data into variable\n",
    "data = pd.read_csv('ARIAL.csv')\n",
    "\n",
    "# selects which columns to keep m_label and all the r{x} c{y}\n",
    "columns_to_keep = ['m_label']\n",
    "columns_to_keep += [f'r{r}c{c}' for r in range(0, 20) for c in range(0, 20)]\n",
    "filtered_data = data[columns_to_keep]\n",
    "\n",
    "# funtion that transforms dataframe returns 2 numpy arrays\n",
    "# x sample x 20 x 20 has pixel val, y #samples x 1 array has ascii for each char\n",
    "def transform_data(data_frame):\n",
    "    # extract the pixel val and normalize data\n",
    "    # . values converts from pandas to numpy array\n",
    "    Xs = data_frame[[f'r{r}c{c}' for r in range(0, 20) for c in range(0, 20)]].values\n",
    "    # makes it samples x 20 x 20 D / 256.0\n",
    "    Xs = Xs.reshape(-1, 20, 20) / 256.0\n",
    "\n",
    "    # extrac the ascii value for each char\n",
    "    Ys = data_frame['m_label'].values\n",
    "    # makes samples# x 1 Dim\n",
    "    Ys = Ys.reshape(-1, 1)\n",
    "\n",
    "    return Xs, Ys\n",
    "\n",
    "Xs, Ys = transform_data(filtered_data)\n",
    "\n",
    "# dictionary for label conversion - using set (collection of unique elements)\n",
    "unique_chars = sorted(set(filtered_data['m_label']))\n",
    "# maps each char to unique index\n",
    "char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "# maps each index back to char\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
    "\n",
    "# convert labels to indices\n",
    "Ys = np.array([char_to_index[char] for char in Ys.flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2: Build a Pytorch network</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [200], Loss: 7.0237\n",
      "Epoch [1], Step [400], Loss: 6.1569\n",
      "Epoch [1], Step [600], Loss: 5.6415\n",
      "Epoch [1], Step [800], Loss: 5.2335\n",
      "Epoch [2], Step [200], Loss: 4.6029\n",
      "Epoch [2], Step [400], Loss: 4.2131\n",
      "Epoch [2], Step [600], Loss: 3.8564\n",
      "Epoch [2], Step [800], Loss: 3.4528\n",
      "Epoch [3], Step [200], Loss: 2.8372\n",
      "Epoch [3], Step [400], Loss: 2.6052\n",
      "Epoch [3], Step [600], Loss: 2.4387\n",
      "Epoch [3], Step [800], Loss: 2.2733\n",
      "Epoch [4], Step [200], Loss: 1.8835\n",
      "Epoch [4], Step [400], Loss: 1.9062\n",
      "Epoch [4], Step [600], Loss: 1.8397\n",
      "Epoch [4], Step [800], Loss: 1.7819\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Build a Pytorch network where its archecture is\n",
    "    # Convolution 2D layer (relu)\n",
    "    # Max pooling layer\n",
    "    # Convolution, another Max pooling\n",
    "    # Dense layer (relu), dense layer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Tensor is numpy multi dim array\n",
    "# Convert data to PyTorch tensors\n",
    "Xs = torch.tensor(Xs, dtype=torch.float32).reshape(-1, 1, 20, 20) # between 0 and 1\n",
    "Ys = torch.tensor(Ys, dtype=torch.long) # can be long int\n",
    "\n",
    "# So we can iterate over batches\n",
    "dataset = TensorDataset(Xs, Ys)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Network as a class with a constructor and forward method\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        # parent class\n",
    "        super(Net, self).__init__()\n",
    "        # 1d input, 6 outputs and 3 x 3 pixels kernel filter\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        # kernel size of 2, reduces spatial dim by half, with stride of 2 for 2x2 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        #conv1 output - ((input size - kernel size + 2 x Padding) / Stride)+1\n",
    "        # 20 - 3 / 1 + 1 -- 18 x 18\n",
    "        # after first pooling -- 9 x 9 size instead of 18 x 18\n",
    "        # 6 from the 6 output layer in the 1st convolution layer\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # 9 - 3 / 1 + 1 -- 7 x 7\n",
    "        # after second layer of pooling - 3 x 3\n",
    "        # first dense layer has 16 * 3 * 3 input features and 120 neurons (output features)\n",
    "        # after second pooling layer, we have 16 channels 3 x 3\n",
    "        self.fc1 = nn.Linear(16 * 3 * 3, 120)\n",
    "        #self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(120, len(unique_chars))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))# conv1 -> relu -> max pool\n",
    "        x = self.pool(F.relu(self.conv2(x)))# conv2 -> relu -> max pool\n",
    "        x = x.view(-1, 16 * 3 * 3)# flattens the tensor back to 1 D\n",
    "        x = F.relu(self.fc1(x)) # FC1 -> relu\n",
    "        #x = F.relu(self.fc2(x)) # FC2 -> relu\n",
    "        x = self.fc3(x) # last dense layer\n",
    "        return x\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "# remember that cuda is using GPU with parallelism\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "net = Net().to(device)\n",
    "# measures error for classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# uses ADAM optimizer to find the best weights\n",
    "optmizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# training function\n",
    "def train(model,train_loader, optmizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            #inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the param gradients\n",
    "            optmizer.zero_grad()\n",
    "            outputs = model(inputs) # predict the output with training data\n",
    "            loss = criterion(outputs, labels) # see how well we did\n",
    "            loss.backward() # see how to change weight to do better\n",
    "            optmizer.step() # actually changes the weights\n",
    "            running_loss += loss.item()\n",
    "            # prints every 200 batch statistics\n",
    "            if i % 200 == 199:\n",
    "                print(f'Epoch [{epoch + 1}], Step [{i + 1}], Loss: {running_loss / 200:.4f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "\n",
    "train(net,train_loader, optmizer, criterion, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 58.44%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate function\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            images, labels = data\n",
    "            #images, labels = images.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy of the network: { 100 * correct / total:.2f}%')\n",
    "\n",
    "evaluate(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3: Exploration and Evaluation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network using cross validation\n",
    "# (splitting data into training/testing). What is its accuracy?\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# random number is arbitrary\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xs, Ys, test_size=0.2, random_state=42)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, drop_last=True)\n",
    "\n",
    "# function to properly train NN and do a Evaluation with Cross-Validation\n",
    "def validade_CV(model, test_dataset):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            #images, labels = images.to(device), labels.to(device).view(-1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_loss /= len(test_dataset)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Step [200], Loss: 1.5541\n",
      "Epoch [1], Step [400], Loss: 1.5452\n",
      "Epoch [1], Step [600], Loss: 1.5401\n",
      "Epoch [2], Step [200], Loss: 1.3314\n",
      "Epoch [2], Step [400], Loss: 1.3895\n",
      "Epoch [2], Step [600], Loss: 1.3865\n",
      "Epoch [3], Step [200], Loss: 1.1993\n",
      "Epoch [3], Step [400], Loss: 1.2748\n",
      "Epoch [3], Step [600], Loss: 1.2660\n",
      "Epoch [4], Step [200], Loss: 1.1267\n",
      "Epoch [4], Step [400], Loss: 1.1778\n",
      "Epoch [4], Step [600], Loss: 1.1581\n",
      "Finished Training\n",
      "Validation Loss: 58.99%\n"
     ]
    }
   ],
   "source": [
    "train(net,train_loader, optmizer, criterion , 4)\n",
    "validade_CV(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create and train a different topology, adding more convolutiuon layers\n",
    "class NetImproved(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetImproved, self).__init__()\n",
    "        a = 1 # solve for a ...\n",
    "        # 1d input, 6 outputs and 3 x 3 pixels kernel filter\n",
    "        c1Out = 4\n",
    "        c2Out = 16\n",
    "        c3Out = 32\n",
    "        self.conv1 = nn.Conv2d(1, c1Out, 3)\n",
    "        # convoluted layer 1 output -> 20 - 3 + 1 --18 x 18\n",
    "        # first pooling layer -- 9 x 9\n",
    "        self.conv2 = nn.Conv2d(c1Out, c2Out, 3)\n",
    "        # convoluted layer 2 output -> 9 - 3 + 1 -- 7 x 7\n",
    "        # second pooling layer -- 3 x 3\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(c2Out, c3Out, 3)\n",
    "        # convoluted layer 3 output -> 3 - 3 + 1 -- 1\n",
    "        self.pooledOutputSize = c3Out * a * a\n",
    "        #self.fc1 = nn.Linear(self.pooledOutputSize, 120)\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.pooledOutputSize, 120),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        #self.fc2 = nn.Linear(120, len(unique_chars))\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(120, len(unique_chars)),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, self.pooledOutputSize)\n",
    "        x = F.relu(self. fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "net_improved = NetImproved().to(device)\n",
    "optimizer_improved = optim.Adam(net_improved.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(net_improved,train_loader, optimizer_improved, criterion, 4)\n",
    "#validade_CV(net_improved, test_loader)\n",
    "#evaluate(net_improved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
